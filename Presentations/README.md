# Presentations
This course had two series of presentations.

## Evasion Presentations
Held on May 14, 2024.

Suggested papers:

1. [Audio Adversarial Examples: Targeted Attacks on Speech-to-Text](https://arxiv.org/abs/1801.01944)
2. [Perceptual Adversarial Robustness: Defense Against Unseen Threat Models](https://arxiv.org/abs/2006.12655)
3. [Adversarial Examples Are Not Bugs, They Are Features](https://arxiv.org/abs/1905.02175)
4. [Feature Purification: How Adversarial Training Performs Robust Deep Learning](https://arxiv.org/abs/2005.10190)
5. [On Adaptive Attacks to Adversarial Example Defenses](https://nicholas.carlini.com/papers/2020_neurips_adaptiveattacks.pdf)
6. [Fundamental Tradeoffs between Invariance and Sensitivity to Adversarial Perturbations](https://arxiv.org/abs/2002.04599)
7. [Adversarial Training for Free!](https://arxiv.org/pdf/1904.12843.pdf)
8. [Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks](https://proceedings.mlr.press/v119/croce20b/croce20b.pdf)
9. **[Data Augmentation Can Improve Robustness](https://proceedings.neurips.cc/paper/2021/file/fb4c48608ce8825b558ccf07169a3421-Paper.pdf)** (_our presentation_)
10. [Square Attack: a query-efficient black-box adversarial attack via random search](https://arxiv.org/abs/1912.00049)
11. [Detecting Adversarial Examples Is (Nearly) As Hard As Classifying Them](https://proceedings.mlr.press/v162/tramer22a.html)
12. [Adversarial Examples for Malware Detection](https://link.springer.com/chapter/10.1007/978-3-319-66399-9_4)
13. [Increasing Confidence in Adversarial Robustness Evaluations](https://arxiv.org/abs/2206.13991)
14. [On Improving Adversarial Transferability of Vision Transformers](https://arxiv.org/abs/2106.04169)
15. [Query-Efficient Decision-Based Adversarial Attacks via Bayesian Optimization](https://openreview.net/forum?id=beiz51zcm-H)
16. [BERT-based Adversarial Examples for Text Classification](https://arxiv.org/abs/2004.01970)
17. [On the Adversarial Robustness of Multi-Modal Foundation Models](https://openaccess.thecvf.com/content/ICCV2023W/AROW/papers/Schlarmann_On_the_Adversarial_Robustness_of_Multi-Modal_Foundation_Models_ICCVW_2023_paper.pdf)
18. [Segment and Complete: Defending Object Detectors against Adversarial Patch Attacks with Robust Patch Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Segment_and_Complete_Defending_Object_Detectors_Against_Adversarial_Patch_Attacks_CVPR_2022_paper.pdf)


## Differential Privacy / Large Language Models
Held on June 29, 2024.

Suggested papers:

1. [Extracting Training Data from Large Language Models](https://www.usenix.org/system/files/sec21-carlini-extracting.pdf)
2. [Large Language Models Can Be Strong Differentially Private Learners](https://openreview.net/pdf?id=bVuP3ltATMz)
3. [Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks](https://people.cs.uchicago.edu/~ravenben/publications/pdf/backdoor-sp19.pdf)
4. [Trojaning Attack on Neural Networks](https://docs.lib.purdue.edu/cgi/viewcontent.cgi?article=2782&context=cstech)
5. [Reverse-engineering deep ReLU networks](https://arxiv.org/pdf/1910.00744.pdf)
6. [Renyi Differential Privacy](https://arxiv.org/pdf/1702.07476.pdf)
7. [Certified Robustness to Adversarial Examples with Differential Privacy](https://arxiv.org/abs/1802.03471)
8. [Differentially Private Empirical Risk Minimization: Efficient Algorithms and Tight Error Bounds](https://par.nsf.gov/servlets/purl/10092778)
9. [Adversary Instantiation: Lower Bounds for Differentially Private Machine Learning](https://ieeexplore.ieee.org/document/9519424)
10. [Stealing Part of a Production Language Model](https://arxiv.org/abs/2403.06634)
11. [Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!](https://arxiv.org/abs/2310.03693)
12. [AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models](https://openreview.net/forum?id=7Jwpw4qKkb)
13. **[Multilingual Jailbreak Challenges in Large Language Models](https://openreview.net/forum?id=vESNKdEMGp)** (_our presentation_)
14. [Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation](https://openreview.net/forum?id=r42tSSCHPh)
15. [On the Reliability of Watermarks for Large Language Models](https://openreview.net/forum?id=DEJIDCmWOz)
16. [On Evaluating Adversarial Robustness of Large Vision-Language Models](https://proceedings.neurips.cc/paper_files/paper/2023/file/a97b58c4f7551053b0512f92244b0810-Paper-Conference.pdf)
17. [Fast Adversarial Attacks on Language Models In One GPU Minute](https://arxiv.org/abs/2402.15570)
