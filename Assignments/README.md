# Assignments 
This course had six assignments.

| HW\# | Topic |
| :-: | :----: |
| HW1 | Neural Networks |
| HW2 | CNNs and FGSM |
| HW3 | PGD and CW Attacks |
| HW4 | UAP and JbDA |
| HW5 | NES and Poisoning |
| HW6 | DP and LLMs |

## Papers 
| HW\# | Topic | Paper |
| :--: | :---: | :---: | 
| HW4 | Black-Box AEs | [Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks](https://arxiv.org/abs/1809.02861)<br>[Delving Into Transferable Adversarial Examples and Black-Box Attacks](https://arxiv.org/abs/1611.02770)<br>[Prior Convictions: Black-Box Adversarial Attacks with Bandits and Priors](https://arxiv.org/abs/1807.07978) |
| HW6 | Differential Privacy | [The Algorithmic Foundations of Differential Privacy](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf)<br>[Privacy-preserving logistic regression](https://papers.nips.cc/paper_files/paper/2008/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf)<br>[The Complexity of Differential Privacy](https://privacytools.seas.harvard.edu/files/privacytools/files/complexityprivacy_1.pdf) |
| HW6 | LLM Security | [Jailbreaking Black Box Large Language Models in Twenty Queries](https://arxiv.org/abs/2310.08419)<br>[AutoDAN: Interpertable Gradient-Based Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2310.15140v2)<br>[SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks](https://arxiv.org/abs/2310.03684) |

## 6. Differential Privacy, Large Language Models
