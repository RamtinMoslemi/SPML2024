# RIML
Adversarial Robust Learning and its Generalization Issues 

## Readings
Following papers are about **Neural Tangent Kernels** (NTK):
1.  [Neural Tangent Kernel: Convergence and Generalization in Neural Networks](https://arxiv.org/abs/1806.07572)
2.  [Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent](https://arxiv.org/abs/1902.06720)
3.  [Deep Learning Versus Kernel Learning: An Empirical Study of Loss Landscape Geometry and the Time Evolution of the Neural Tangent Kernel](https://arxiv.org/abs/2010.15110)
4.  [Evolution of Neural Tangent Kernels under Benign and Adversarial Training](https://arxiv.org/abs/2210.12030)
5.  [Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK Approach](https://arxiv.org/abs/2310.06112)
6.  [Rethinking Adversarial Training with Neural Tangent Kernel](https://arxiv.org/pdf/2312.02236)

Following papers are about **Robust Overfitting**:
1.  [Overfitting in Adversarially Robust Deep Learning](https://arxiv.org/abs/2002.11569)

Following papers are about **Catastrophic Overfitting** (CO):

1.  [Fast is Better Than Free: Revisiting Adversarial Training](https://arxiv.org/abs/2001.03994)
2.  [Towards Understanding Fast Adversarial Training](https://arxiv.org/abs/2006.03089)
3.  [Understanding and Improving Fast Adversarial Training](https://arxiv.org/abs/2007.02617)
4.  [Understanding Catastrophic Overfitting in Single-Step Adversarial Training](https://arxiv.org/abs/2010.01799)
5.  [ZeroGrad: Mitigating and Explaining Catastrophic Overfitting in FGSM Adversarial Training](https://arxiv.org/abs/2103.15476)
6.  [Reliably Fast Adversarial Training via Latent Adversarial Perturbation](https://arxiv.org/abs/2104.01575)
7.  [Understanding Catastrophic Overfitting in Adversarial Training](https://arxiv.org/abs/2105.02942)
8.  [Boosting Fast Adversarial Training with Learnable Adversarial Initialization](https://arxiv.org/abs/2110.05007)
9.  [Subspace Adversarial Training](https://arxiv.org/abs/2111.12229)
10. [Revisiting and Advancing Fast Adversarial Training Through The Lens of Bi-Level Optimization](https://arxiv.org/abs/2112.12376)
11. [Make Some Noise: Reliable and Efficient Single-Step Adversarial Training](https://arxiv.org/abs/2202.01181)
12. [Catastrophic Overfitting Can Be Induced with Discriminative Non-Robust Features](https://arxiv.org/abs/2206.08242)
13. [Efficient Local Linearity Regularization to Overcome Catastrophic Overfitting](https://arxiv.org/abs/2401.11618)
