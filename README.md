# SPML2024
Assignments and Presentations for Security and Privacy in Machine Learning 2024

## Course Info
The public page of the course is [here](https://spml2024.github.io). The first part of the course was focused on **Adversarial Examples (AE)** and the second part focused on a number of topics including **Data Poisoning**, **Model Extraction (ME)**, **Differential Privacy (DP)**, and the **Security of Large Language Models (LLM)**.

## Readings
The following papers were covered in, and were a part of the course. 

| Topic | Paper |
| :---: | :---: |
| **Introduction** | [Towards the Science of Security and Privacy in Machine Learning](https://arxiv.org/abs/1611.03814) |
| **AE Generating Methods** | [Intriguing Properties of Neural Networks](https://arxiv.org/abs/1312.6199)<br>[Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)<br>[Towards Evaluating the Robustness of Neural Networks](https://arxiv.org/abs/1608.04644)<br>[Universal Adversarial Perturbations](https://arxiv.org/abs/1610.08401)<br>[Adversarial Patch](https://arxiv.org/abs/1712.09665) |
| **Defenses Against AEs**| [Towards Deep Learning Models Resistant to Adversarial Attacks](https://arxiv.org/abs/1706.06083)<br>[Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples](http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf)<br>[Certified Adversarial Robustness via Randomized Smoothing](https://arxiv.org/abs/1902.02918)<br>[Provably robust deep learning via adversarially trained smoothed classifiers](https://proceedings.neurips.cc/paper/2019/file/3a24b25a7b092a252166a1641ae953e7-Paper.pdf) |
| **Black-Box AEs** | [Practical Black-Box Attacks against Machine Learning](https://www.cs.purdue.edu/homes/bb/2020-fall-cs590bb/docs/at/attacks-against-machine-learning.pdf)<br>[ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks without Training Substitute Models](https://dl.acm.org/doi/abs/10.1145/3128572.3140448)<br>[Black-box Adversarial Attacks with Limited Queries and Information](https://arxiv.org/abs/1804.08598) |
| **Poisoning** | [BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain](https://arxiv.org/abs/1708.06733)<br>[Clean-Label Backdoor Attacks](https://people.csail.mit.edu/madry/lab/cleanlabel.pdf)<br>[Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks](https://arxiv.org/abs/1804.00792)<br>[Deep Partition Aggregation: Provable Defense against General Poisoning Attacks](https://arxiv.org/abs/2006.14768) |
| **Model Extraction** | [High Accuracy and High Fidelity Extraction of Neural Networks](https://arxiv.org/abs/1909.01838)<br>[Knockoff Nets: Stealing Functionality of Black-Box Models](https://arxiv.org/abs/1812.02766)<br>[Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring](https://arxiv.org/abs/1802.04633) |
| **Privacy** | [Membership Inference Attacks against Machine Learning Models](https://arxiv.org/abs/1610.05820)<br>[Passive and Active White-box Inference Attacks against Centralized and Federated Learning](https://arxiv.org/abs/1812.00910)<br>[The Algorithmic Foundations of Differential Privacy](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf)<br>[Deep Learning with Differential Privacy](https://arxiv.org/abs/1607.00133)<br>[Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data](https://arxiv.org/abs/1610.05755) |
| **LLM Security** | [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043) |

### Additional Papers
The papers that were were covered in the:
-  first series of presentations, which focus on **Adversarial Examples**, can be found [here](https://github.com/RamtinMoslemi/SPML2024/tree/main/Presentations#evasion-presentations).
-  second series of presentations, which focus on **Model Extraction**, **Privacy** and **LLM Security**, can be found [here](https://github.com/RamtinMoslemi/SPML2024/tree/main/Presentations#differential-privacy--large-language-models). 
-  homework assignments, which focus on **Black-Box AEs**, **DP** and **LLM Security**, can be found [here](https://github.com/RamtinMoslemi/SPML2024/tree/main/Assignments#papers).

# Other Resources
The following supplementary resouces can help you learn more or fill your knwoledge gaps:
-  You can find more about **Adversarial Examples** from [this blog](https://openai.com/index/attacking-machine-learning-with-adversarial-examples/) from OpenAI.
-  To learn more about **Adversarial Machine Learning** you can check out [this article](https://en.wikipedia.org/wiki/Adversarial_machine_learning) from Wikipedia.
-  The guest lecture given by [Ian Goodfellow](https://www.iangoodfellow.com) for [Stanford CS231n: Deep Learning for Computer Vision](https://cs231n.stanford.edu) titled [Adversarial Examples and Adversarial Training](https://youtu.be/CIfsB_EYsVI?feature=shared) is another great resource.
-  While there aren't any textbooks that specifically focus on these topics, the [Robust Optimization](https://press.princeton.edu/books/hardcover/9780691143682/robust-optimization?srsltid=AfmBOorod7ku6JCwVHZSlgmJzkxa-2BztqRskybl5Lp_rLBCCSOdLhmj) textbook covers some interesting related topics.
-  You can check out [CS 860 - Algorithms for Private Data Analysis](http://www.gautamkamath.com/CS860-fa2020.html) instructed by [Gautam Kamath](http://www.gautamkamath.com) for [A Course In Differential Privacy](https://youtube.com/playlist?list=PLmd_zeMNzSvRRNpoEWkVo6QY_6rR3SHjp&feature=shared).
-  The [Privacy Preserving Machine Learning](http://researchers.lille.inria.fr/abellet/teaching/private_machine_learning_course.html) course taught by [Aur√©lien Bellet](http://researchers.lille.inria.fr/abellet/) is another great resource.
-  You can find a lot of interesting stuff on [Nicholas Carlini's personal page](https://nicholas.carlini.com).
