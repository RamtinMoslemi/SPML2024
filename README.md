# SPML2024
Assignments and Presentations for Security and Privacy in Machine Learning 2024

## Course Info
The public page of the course is [here](https://spml2024.github.io). The first part of the course was focused on **Adversarial Examples (AE)** and the second part focused on a number of topics including **Data Poisoning**, **Model Extraction (ME)**, **Differential Privacy (DP)**, and the **Security of Large Language Models (LLM)**.

## Readings
The following papers were covered in, and were a part of the course. 

| Topic | Paper |
| :---: | :---: |
| **Introduction** | [Towards the Science of Security and Privacy in Machine Learning](https://arxiv.org/abs/1611.03814) |
| **AE Generating Methods** | [Intriguing Properties of Neural Networks](https://arxiv.org/abs/1312.6199)<br>[Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)<br>[Towards Evaluating the Robustness of Neural Networks](https://arxiv.org/abs/1608.04644)<br>[Universal Adversarial Perturbations](https://arxiv.org/abs/1610.08401)<br>[Adversarial Patch](https://arxiv.org/abs/1712.09665) |
| **Defenses Against AEs**| [Towards Deep Learning Models Resistant to Adversarial Attacks](https://arxiv.org/abs/1706.06083)<br>[Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples](http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf)<br>[Certified Adversarial Robustness via Randomized Smoothing](https://arxiv.org/pdf/1902.02918)<br>[Provably robust deep learning via adversarially trained smoothed classifiers](https://proceedings.neurips.cc/paper/2019/file/3a24b25a7b092a252166a1641ae953e7-Paper.pdf) |
| **Black-Box AEs** | [Practical Black-Box Attacks against Machine Learning](https://www.cs.purdue.edu/homes/bb/2020-fall-cs590bb/docs/at/attacks-against-machine-learning.pdf)<br>[ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks without Training Substitute Models](https://dl.acm.org/doi/pdf/10.1145/3128572.3140448)<br>[Black-box Adversarial Attacks with Limited Queries and Information](https://arxiv.org/pdf/1804.08598) |
| **Poisoning** | [BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain](https://arxiv.org/abs/1708.06733)<br>[Clean-Label Backdoor Attacks](https://people.csail.mit.edu/madry/lab/cleanlabel.pdf)<br>[Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks](https://arxiv.org/abs/1804.00792)<br>[Deep Partition Aggregation: Provable Defense against General Poisoning Attacks](https://arxiv.org/pdf/2006.14768) |
| **Model Extraction** | [High Accuracy and High Fidelity Extraction of Neural Networks](https://arxiv.org/abs/1909.01838)<br>[Knockoff Nets: Stealing Functionality of Black-Box Models](https://arxiv.org/abs/1812.02766)<br>[Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring](https://arxiv.org/pdf/1802.04633) |
| **Privacy** | [Membership Inference Attacks against Machine Learning Models](https://arxiv.org/abs/1610.05820)<br>[Passive and Active White-box Inference Attacks against Centralized and Federated Learning](https://arxiv.org/abs/1812.00910)<br>[The Algorithmic Foundations of Differential Privacy](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf)<br>[Deep Learning with Differential Privacy](https://arxiv.org/abs/1607.00133)<br>[Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data](https://arxiv.org/abs/1610.05755) |
| **LLM Security** | [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043) |

## Additional Papers
The papers that were were covered in the:
-  first series of presentations, which focus on **Adversarial Examples**, can be found [here](https://github.com/RamtinMoslemi/SPML2024/tree/main/Presentations#evasion-presentations).
-  second series of presentations, which focus on **Model Extraction**, **Privacy** and **LLM Security**, can be found [here](https://github.com/RamtinMoslemi/SPML2024/tree/main/Presentations#differential-privacy--large-language-models). 
-  homework assignments, which focus on **Black-Box AEs** and **LLM Security**, can be found [here](https://github.com/RamtinMoslemi/SPML2024/tree/main/Assignments#papers).
